{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "9c48e94e",
            "metadata": {},
            "source": [
                "#### Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "f91235bd-faa2-4ec4-8049-228f94df8fd3",
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "\n",
                "class NaiveBayes(object):\n",
                "    \"\"\" Gaussian Naive Bayes model\n",
                "    \n",
                "    @attrs:\n",
                "        n_classes:    the number of classes\n",
                "        attr_dist:    a 2D (n_classes x n_attributes) NumPy array of the attribute distributions\n",
                "        label_priors: a 1D NumPy array of the priors distribution\n",
                "        means:        mean per class and feature\n",
                "        vars:         variance per class and feature\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(self, n_classes):\n",
                "        \"\"\" Initializes a NaiveBayes model with n_classes. \"\"\"\n",
                "        self.n_classes = n_classes\n",
                "        self.means = None          \n",
                "        self.vars = None  \n",
                "        self.label_priors = None\n",
                "\n",
                "    def train(self, X_train, y_train):\n",
                "        \"\"\" Trains the model, using maximum likelihood estimation.\n",
                "        For Gaussian NB: estimate class priors, means, and variances.\n",
                "        @params:\n",
                "            X_train: a 2D (n_examples x n_attributes) numpy array\n",
                "            y_train: a 1D (n_examples) numpy array\n",
                "        @return:\n",
                "            a tuple consisting of:\n",
                "                1) a 2D numpy array of the attribute distributions\n",
                "                2) a 1D numpy array of the priors distribution\n",
                "        \"\"\"\n",
                "        n_samples, n_features = X_train.shape\n",
                "\n",
                "        # ---- 1. Estimate class priors----\n",
                "        counts = np.bincount(y_train, minlength=self.n_classes)\n",
                "        self.label_priors = counts / n_samples  \n",
                "\n",
                "        # ---- 2. Estimate means and variances for each class and feature ----\n",
                "        self.means = np.zeros((self.n_classes, n_features))\n",
                "        self.vars = np.zeros((self.n_classes, n_features))\n",
                "\n",
                "        for c in range(self.n_classes):\n",
                "            X_c = X_train[y_train == c]  \n",
                "            self.means[c, :] = np.mean(X_c, axis=0)\n",
                "            self.vars[c, :] = np.var(X_c, axis=0) + 1e-9\n",
                "\n",
                "        return self.means, self.vars, self.label_priors\n",
                "\n",
                "    def predict(self, inputs):\n",
                "        \"\"\" Outputs a predicted label for each input in inputs.\n",
                "            Remember to convert to log space to avoid overflow/underflow\n",
                "            errors!\n",
                "\n",
                "        @params:\n",
                "            inputs: a 2D NumPy array containing inputs\n",
                "        @return:\n",
                "            a 1D numpy array of predictions\n",
                "        \"\"\"\n",
                "        n_samples = inputs.shape[0]\n",
                "        log_priors = np.log(self.label_priors) \n",
                "        predictions = np.zeros(n_samples, dtype=int)\n",
                "        for i in range(n_samples):\n",
                "            x = inputs[i]\n",
                "            log_probs = np.zeros(self.n_classes)\n",
                "            for c in range(self.n_classes):\n",
                "                mean_c=self.means[c]\n",
                "                var_c=self.vars[c]\n",
                "                log_likelihood = -0.5 * np.sum(\n",
                "                    np.log(2 * np.pi * var_c) + ((x - mean_c) ** 2) / var_c\n",
                "                )\n",
                "                log_probs[c] = log_priors[c] + log_likelihood\n",
                "            joint_probs = np.exp(log_probs)     \n",
                "            evidence = np.sum(joint_probs)       \n",
                "            posterior = joint_probs / evidence      \n",
                "            predictions[i] = np.argmax(posterior)\n",
                "        return predictions\n",
                "    def accuracy(self, X_test, y_test):\n",
                "        \"\"\" Outputs the accuracy of the trained model on a given dataset (data).\n",
                "\n",
                "        @params:\n",
                "            X_test: a 2D numpy array of examples\n",
                "            y_test: a 1D numpy array of labels\n",
                "        @return:\n",
                "            a float number indicating accuracy (between 0 and 1)\n",
                "        \"\"\"\n",
                "\n",
                "        # TODO\n",
                "        predictions = self.predict(X_test)\n",
                "        correct = np.sum(predictions == y_test)\n",
                "        total = y_test.shape[0]\n",
                "        accuracy = correct / total\n",
                "        return accuracy\n",
                "        \n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e28029a3",
            "metadata": {},
            "source": [
                "#### Check Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "e8801d0d-a2f7-4c98-a56b-d81979e4d014",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Success!\n"
                    ]
                }
            ],
            "source": [
                "import numpy as np\n",
                "import pytest\n",
                "\n",
                "\"\"\"Two-class simple sanity check with held-out test set.\"\"\"\n",
                "X_train = np.array([\n",
                "    [0.0, 1.0],  # class 0\n",
                "    [0.0, 3.0],  # class 0\n",
                "    [2.0, 1.0],  # class 1\n",
                "    [2.0, 3.0],  # class 1\n",
                "])\n",
                "y_train = np.array([0, 0, 1, 1])\n",
                "\n",
                "nb = NaiveBayes(n_classes=2)\n",
                "means, vars_, priors = nb.train(X_train, y_train)\n",
                "\n",
                "# Priors: two samples in each class -> 0.5, 0.5\n",
                "assert np.allclose(priors, np.array([0.5, 0.5]))\n",
                "\n",
                "# Means:\n",
                "# class 0: X_c0 = [[0,1], [0,3]] -> mean = [0, 2]\n",
                "# class 1: X_c1 = [[2,1], [2,3]] -> mean = [2, 2]\n",
                "expected_means = np.array([[0.0, 2.0],\n",
                "                           [2.0, 2.0]])\n",
                "assert np.allclose(means, expected_means)\n",
                "\n",
                "# Variances (population, N in denominator):\n",
                "# class 0, feature 0: values [0, 0] -> var = 0\n",
                "# class 0, feature 1: values [1, 3] -> mean = 2, var = ((1-2)**2 + (3-2)**2)/2 = 1\n",
                "# class 1, feature 0: values [2, 2] -> var = 0\n",
                "# class 1, feature 1: values [1, 3] -> same as above = 1\n",
                "# added 1e-9 for numerical stability in the implementation.\n",
                "expected_vars = np.array([[0.0, 1.0],\n",
                "                          [0.0, 1.0]]) + 1e-9\n",
                "assert np.allclose(vars_, expected_vars)\n",
                "\n",
                "# Held-out test set.\n",
                "X_test = np.array([\n",
                "    [0.0, 1.5],  # class 0\n",
                "    [2.0, 2.5],  # class 1\n",
                "])\n",
                "y_test = np.array([0, 1])\n",
                "\n",
                "y_pred = nb.predict(X_test)\n",
                "assert np.array_equal(y_pred, y_test)\n",
                "\n",
                "acc = nb.accuracy(X_test, y_test)\n",
                "assert acc == 1.0\n",
                "print(\"Success!\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "f2e9c20d-0c2a-4e21-9f6e-abcdef123456",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Success!\n"
                    ]
                }
            ],
            "source": [
                "\"\"\"Edge case: zero-variance feature and imbalanced classes, with held-out test set.\"\"\"\n",
                "# Feature 0 is always 1.0 (zero variance), feature 1 separates classes.\n",
                "X_train = np.array([\n",
                "    [1.0, 0.0],  # class 0\n",
                "    [1.0, 0.2],  # class 0\n",
                "    [1.0, 0.1],  # class 0\n",
                "    [1.0, 1.0],  # class 1\n",
                "])\n",
                "y_train = np.array([0, 0, 0, 1])\n",
                "\n",
                "nb = NaiveBayes(n_classes=2)\n",
                "means, vars_, priors = nb.train(X_train, y_train)\n",
                "\n",
                "# Class priors should reflect the 3:1 imbalance.\n",
                "assert np.allclose(priors, np.array([0.75, 0.25]))\n",
                "\n",
                "# Zero-variance feature should have a small positive variance due to 1e-9.\n",
                "assert np.all(vars_[:, 0] > 0)\n",
                "\n",
                "# Held-out test set (use points consistent with the zero-variance feature).\n",
                "X_test = np.array([\n",
                "    [1.0, 0.05],  # class 0\n",
                "    [1.0, 0.9],   # class 1 (matches the lone class-1 sample)\n",
                "])\n",
                "y_test = np.array([0, 0])\n",
                "\n",
                "y_pred = nb.predict(X_test)\n",
                "\n",
                "assert np.array_equal(y_pred, y_test)\n",
                "\n",
                "acc = nb.accuracy(X_test, y_test)\n",
                "assert acc == 1.0\n",
                "print(\"Success!\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "a7d0f3b1-6e8a-4ad9-b7b2-fedcba012345",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Success!\n"
                    ]
                }
            ],
            "source": [
                "\"\"\"Multi-class (>2) sanity check with held-out test set.\"\"\"\n",
                "X_train = np.array([\n",
                "    [0.0, 0.0],  # class 0\n",
                "    [0.0, 1.0],  # class 0\n",
                "    [1.0, 0.0],  # class 1\n",
                "    [1.0, 1.0],  # class 1\n",
                "    [2.0, 0.0],  # class 2\n",
                "    [2.0, 1.0],  # class 2\n",
                "])\n",
                "y_train = np.array([0, 0, 1, 1, 2, 2])\n",
                "\n",
                "nb = NaiveBayes(n_classes=3)\n",
                "means, vars_, priors = nb.train(X_train, y_train)\n",
                "\n",
                "# Priors: two samples in each of the three classes -> 1/3 each.\n",
                "expected_priors = np.array([1/3, 1/3, 1/3])\n",
                "assert np.allclose(priors, expected_priors)\n",
                "\n",
                "# Means for each class:\n",
                "# class 0: [[0,0],[0,1]] -> mean [0, 0.5]\n",
                "# class 1: [[1,0],[1,1]] -> mean [1, 0.5]\n",
                "# class 2: [[2,0],[2,1]] -> mean [2, 0.5]\n",
                "expected_means = np.array([\n",
                "    [0.0, 0.5],\n",
                "    [1.0, 0.5],\n",
                "    [2.0, 0.5],\n",
                "])\n",
                "assert np.allclose(means, expected_means)\n",
                "\n",
                "# Variances (population) for each class:\n",
                "# feature 0 is constant within each class -> var = 0 (then +1e-9)\n",
                "# feature 1 has values [0,1] in every class -> mean 0.5, var = 0.25\n",
                "expected_vars = np.array([\n",
                "    [0.0, 0.25],\n",
                "    [0.0, 0.25],\n",
                "    [0.0, 0.25],\n",
                "]) + 1e-9\n",
                "assert np.allclose(vars_, expected_vars)\n",
                "\n",
                "# Held-out test set.\n",
                "X_test = np.array([\n",
                "    [0.0, 0.2],  # class 0\n",
                "    [1.0, 0.8],  # class 1\n",
                "    [2.0, 0.2],  # class 2\n",
                "])\n",
                "y_test = np.array([0, 1, 2])\n",
                "\n",
                "y_pred = nb.predict(X_test)\n",
                "assert np.array_equal(y_pred, y_test)\n",
                "\n",
                "acc = nb.accuracy(X_test, y_test)\n",
                "assert acc == 1.0\n",
                "print(\"Success!\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "data2060",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
